{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from typing import List\n",
        "import base64\n",
        "from fastapi import FastAPI, Request\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "class VLMManager:\n",
        "    def __init__(self):\n",
        "        # Load pre-trained ResNet50 model for image feature extraction\n",
        "        self.resnet = ResNet50(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
        "\n",
        "        # Freeze the layers in the ResNet model\n",
        "        for layer in self.resnet.layers:\n",
        "            layer.trainable = False\n",
        "\n",
        "        # Define the textual input\n",
        "        self.text_input = Input(shape=(None,))\n",
        "\n",
        "        # Define the embedding layer for textual input\n",
        "        vocab_size = 10000  # Example vocabulary size\n",
        "        embedding_dim = 256\n",
        "        self.embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(self.text_input)\n",
        "\n",
        "        # Define the image input\n",
        "        self.image_input = Input(shape=(224, 224, 3))\n",
        "\n",
        "        # Extract features from the image using ResNet\n",
        "        image_features = self.resnet(self.image_input)\n",
        "        image_features = tf.keras.layers.GlobalAveragePooling2D()(image_features)\n",
        "\n",
        "        # Concatenate visual and textual features\n",
        "        combined_features = Concatenate()([image_features, self.embedding_layer])\n",
        "\n",
        "        # Define LSTM layer to generate captions\n",
        "        lstm_units = 256\n",
        "        lstm = LSTM(units=lstm_units)(combined_features)\n",
        "        output = Dense(vocab_size, activation='softmax')(lstm)\n",
        "\n",
        "        # Create the VLM model\n",
        "        self.model = Model(inputs=[self.image_input, self.text_input], outputs=output)\n",
        "        self.model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "    def identify(self, image: bytes, caption: str) -> List[int]:\n",
        "        img = Image.open(io.BytesIO(image))\n",
        "        image_width, image_height = img.size\n",
        "        bbox = [image_width // 4, image_height // 4, image_width // 2, image_height // 2]\n",
        "        return bbox\n",
        "\n",
        "app = FastAPI()\n",
        "vlm_manager = VLMManager()\n",
        "\n",
        "@app.get(\"/health\")\n",
        "def health():\n",
        "    return {\"message\": \"health ok\"}\n",
        "\n",
        "@app.post(\"/identify\")\n",
        "async def identify(request: Request):\n",
        "    \"\"\"\n",
        "    Performs Object Detection and Identification given an image frame and a text query.\n",
        "    \"\"\"\n",
        "    # Get base64 encoded string of image, convert back into bytes\n",
        "    input_json = await request.json()\n",
        "\n",
        "    predictions = []\n",
        "    for instance in input_json[\"instances\"]:\n",
        "        # Each is a dict with one key \"b64\" and the value as a b64 encoded string\n",
        "        image_bytes = base64.b64decode(instance[\"b64\"])\n",
        "\n",
        "        # Perform identification using VLMManager\n",
        "        bbox = vlm_manager.identify(image_bytes, instance[\"caption\"])\n",
        "        predictions.append(bbox)\n",
        "\n",
        "    return {\"predictions\": predictions}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "pip install \"fastapi[all]\"\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
